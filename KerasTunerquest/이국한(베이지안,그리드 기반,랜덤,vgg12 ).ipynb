{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 미션\n",
        "\n",
        "<aside>\n",
        "📌\n",
        "\n",
        "4가지 이상의 하이퍼파라미터를 조정하여 최적의 모델을 찾아봅시다.\n",
        "\n",
        "- 최적의 파라미터 수치\n",
        "- 해당 모델 평가 결과 : val_accuracy, val_loss\n",
        "</aside>\n",
        "\n",
        "### 하이퍼파라미터\n",
        "\n",
        "- Unit size\n",
        "- Kernel size\n",
        "- Dropout rate\n",
        "- Optimizer\n",
        "- Learning rate\n",
        "- Activation functions\n",
        "- Batch size\n",
        "- Regularization\n",
        "- Strides\n",
        "- Padding\n",
        "- Conv, 풀링 Layer 갯수\n",
        "- …등"
      ],
      "metadata": {
        "id": "CxoR_wjhXWX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UavDz2qRXsqJ",
        "outputId": "52ea85c8-1d44-44ca-c5ef-ef7186b3e589"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EHxz3pv9XNvl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7pe_vX_XPiV",
        "outputId": "d54dc1bd-8f3c-41ec-b6d6-b4e01975e8fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "06HOWWQeZuM2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(32, 32, 3)))\n",
        "\n",
        "    # Conv2D 레이어 추가\n",
        "    for i in range(hp.Int(\"conv_layers\", 2, 4)):  # 2~4개의 Conv 레이어\n",
        "        model.add(\n",
        "            layers.Conv2D(\n",
        "                filters=hp.Choice(f\"filters_{i}\", [32, 64, 128, 256]),\n",
        "                kernel_size=hp.Choice(\"kernel_size\", [3, 5]),\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        # MaxPooling2D에 padding=\"same\" 추가 및 크기 확인\n",
        "        if model.layers[-1].output_shape[1] >= 2 and model.layers[-1].output_shape[2] >= 2:\n",
        "            model.add(layers.MaxPooling2D(pool_size=2, padding=\"same\"))\n",
        "        else:\n",
        "            print(\"입력 크기가 너무 작아 MaxPooling2D를 생략합니다.\")\n",
        "            break\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense 레이어 추가\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            units=hp.Int(\"dense_units\", min_value=64, max_value=512, step=64),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dropout(rate=hp.Float(\"dropout_rate\", 0.2, 0.5, step=0.1)))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    # 컴파일 설정\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4, 1e-5])\n",
        "        ),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 베이지안 최적화 설정\n",
        "tuner = kt.BayesianOptimization(\n",
        "    hypermodel=build_model,  # 하이퍼모델 함수\n",
        "    objective=\"val_accuracy\",  # 최적화 목표\n",
        "    max_trials=15,  # 최대 시도 횟수\n",
        "    num_initial_points=3,  # 초기 랜덤 시도 수\n",
        "    alpha=0.0001,  # 탐색 공간의 균형을 조절\n",
        "    beta=2.6,  # 탐색 강도를 조절\n",
        "    seed=42,  # 랜덤 시드\n",
        "    directory=\"bayesian_tuning_cifar10\",  # 결과 저장 디렉토리\n",
        "    project_name=\"cifar10_bayesian\",  # 프로젝트 이름\n",
        ")\n",
        "\n",
        "# 튜닝 시작\n",
        "tuner.search(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=5,  # 한 번의 시도에서 학습할 최대 에포크\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],  # 조기 종료 콜백\n",
        ")\n",
        "\n",
        "# 최적 하이퍼파라미터 가져오기\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"최적 하이퍼파라미터:\")\n",
        "for param, value in best_hps.values.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "# 최적 모델 재학습\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,  # 최적 모델 재학습\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],  # 조기 종료 콜백\n",
        ")\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
        "print(f\"테스트 정확도: {test_acc}\")\n",
        "print(f\"테스트 손실: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mZRoRrcYZvIa",
        "outputId": "89ffef10-57da-4f90-8ba8-9b6d9ecdd45e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from bayesian_tuning_cifar10/cifar10_bayesian/tuner0.json\n",
            "\n",
            "Search: Running Trial #15\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "3                 |2                 |conv_layers\n",
            "128               |128               |filters_0\n",
            "5                 |3                 |kernel_size\n",
            "64                |64                |filters_1\n",
            "128               |256               |dense_units\n",
            "0.4               |0.4               |dropout_rate\n",
            "0.01              |0.001             |learning_rate\n",
            "32                |64                |filters_2\n",
            "32                |None              |filters_3\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n",
            "    model = self._try_build(hp)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n",
            "    model = self._build_hypermodel(hp)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n",
            "    model = self.hypermodel.build(hp)\n",
            "  File \"<ipython-input-19-496e35249a21>\", line 15, in build_model\n",
            "    if model.layers[-1].output_shape[1] >= 2 and model.layers[-1].output_shape[2] >= 2:\n",
            "AttributeError: 'Conv2D' object has no attribute 'output_shape'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"<ipython-input-19-496e35249a21>\", line 15, in build_model\n    if model.layers[-1].output_shape[1] >= 2 and model.layers[-1].output_shape[2] >= 2:\nAttributeError: 'Conv2D' object has no attribute 'output_shape'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-496e35249a21>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# 튜닝 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m tuner.search(\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"Number of consecutive failures exceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"<ipython-input-19-496e35249a21>\", line 15, in build_model\n    if model.layers[-1].output_shape[1] >= 2 and model.layers[-1].output_shape[2] >= 2:\nAttributeError: 'Conv2D' object has no attribute 'output_shape'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼모델 정의\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(32, 32, 3)))\n",
        "\n",
        "    # Conv2D 레이어 추가\n",
        "    for i in range(hp.Int(\"conv_layers\", 2, 4)):  # 2~4개의 Conv 레이어\n",
        "        model.add(\n",
        "            layers.Conv2D(\n",
        "                filters=hp.Choice(f\"filters_{i}\", [32, 64, 128, 256]),\n",
        "                kernel_size=hp.Choice(\"kernel_size\", [3, 5]),\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense 레이어 추가\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            units=hp.Int(\"dense_units\", min_value=64, max_value=512, step=64),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dropout(rate=hp.Float(\"dropout_rate\", 0.2, 0.5, step=0.1)))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    # 컴파일 설정\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4, 1e-5])\n",
        "        ),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 그리드 검색 기반 최적화 (Hyperband)\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=10,  # 최대 에포크 수\n",
        "    factor=3,       # 자원 분배 비율\n",
        "    seed=42,\n",
        "    directory=\"hyperband_tuning_cifar10\",\n",
        "    project_name=\"cifar10_hyperband\",\n",
        ")\n",
        "\n",
        "# 튜닝 시작\n",
        "tuner.search(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],\n",
        ")\n",
        "\n",
        "# 최적 하이퍼파라미터 출력\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"최적 하이퍼파라미터:\")\n",
        "for param, value in best_hps.values.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "# 최적 모델 재학습\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],\n",
        ")\n",
        "\n",
        "# 평가\n",
        "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
        "print(f\"테스트 정확도: {test_acc}\")\n",
        "print(f\"테스트 손실: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcomco85alpI",
        "outputId": "56fd8dc4-fae4-4605-d795-8af3f43a44db"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 24 Complete [00h 01m 37s]\n",
            "val_accuracy: 0.6628999710083008\n",
            "\n",
            "Best val_accuracy So Far: 0.6934999823570251\n",
            "Total elapsed time: 00h 11m 34s\n",
            "최적 하이퍼파라미터:\n",
            "conv_layers: 2\n",
            "filters_0: 128\n",
            "kernel_size: 3\n",
            "filters_1: 256\n",
            "dense_units: 448\n",
            "dropout_rate: 0.30000000000000004\n",
            "learning_rate: 0.001\n",
            "filters_2: 256\n",
            "filters_3: 64\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 1\n",
            "tuner/round: 0\n",
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.4172 - loss: 1.5981 - val_accuracy: 0.6303 - val_loss: 1.0587\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.6413 - loss: 1.0216 - val_accuracy: 0.6795 - val_loss: 0.9310\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7066 - loss: 0.8359 - val_accuracy: 0.7069 - val_loss: 0.8443\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7554 - loss: 0.6916 - val_accuracy: 0.7188 - val_loss: 0.8354\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7975 - loss: 0.5736 - val_accuracy: 0.7233 - val_loss: 0.8380\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.8359 - loss: 0.4708 - val_accuracy: 0.7273 - val_loss: 0.8719\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.8650 - loss: 0.3842 - val_accuracy: 0.7192 - val_loss: 0.9102\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7226 - loss: 0.8984\n",
            "테스트 정확도: 0.7192000150680542\n",
            "테스트 손실: 0.9102368950843811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼모델 정의\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(32, 32, 3)))\n",
        "\n",
        "    # Conv2D 레이어 추가\n",
        "    for i in range(hp.Int(\"conv_layers\", 2, 4)):  # 2~4개의 Conv 레이어\n",
        "        model.add(\n",
        "            layers.Conv2D(\n",
        "                filters=hp.Choice(f\"filters_{i}\", [32, 64, 128, 256]),\n",
        "                kernel_size=hp.Choice(\"kernel_size\", [3, 5]),\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Dense 레이어 추가\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            units=hp.Int(\"dense_units\", min_value=64, max_value=512, step=64),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dropout(rate=hp.Float(\"dropout_rate\", 0.2, 0.5, step=0.1)))\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    # 컴파일 설정\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4, 1e-5])\n",
        "        ),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 랜덤 검색 (Random Search)\n",
        "tuner = kt.RandomSearch(\n",
        "    hypermodel=build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=15,  # 탐색 시도 횟수\n",
        "    seed=42,\n",
        "    directory=\"random_tuning_cifar10\",\n",
        "    project_name=\"cifar10_random\",\n",
        ")\n",
        "\n",
        "# 튜닝 시작\n",
        "tuner.search(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],\n",
        ")\n",
        "\n",
        "# 최적 하이퍼파라미터 출력\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"최적 하이퍼파라미터:\")\n",
        "for param, value in best_hps.values.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "# 최적 모델 재학습\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],\n",
        ")\n",
        "\n",
        "# 평가\n",
        "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
        "print(f\"테스트 정확도: {test_acc}\")\n",
        "print(f\"테스트 손실: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cksdXeaQazsP",
        "outputId": "da155897-5be6-4b4d-d38b-32da8f2d4cd1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 00m 03s]\n",
            "\n",
            "Best val_accuracy So Far: 0.7390999794006348\n",
            "Total elapsed time: 00h 14m 54s\n",
            "최적 하이퍼파라미터:\n",
            "conv_layers: 2\n",
            "filters_0: 256\n",
            "kernel_size: 5\n",
            "filters_1: 256\n",
            "dense_units: 512\n",
            "dropout_rate: 0.2\n",
            "learning_rate: 0.0001\n",
            "filters_2: 64\n",
            "filters_3: 128\n",
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 15ms/step - accuracy: 0.3515 - loss: 1.7848 - val_accuracy: 0.5338 - val_loss: 1.3140\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.5607 - loss: 1.2455 - val_accuracy: 0.6071 - val_loss: 1.1167\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.6311 - loss: 1.0540 - val_accuracy: 0.6450 - val_loss: 0.9983\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.6755 - loss: 0.9420 - val_accuracy: 0.6826 - val_loss: 0.9135\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.7103 - loss: 0.8412 - val_accuracy: 0.6931 - val_loss: 0.8898\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.7438 - loss: 0.7467 - val_accuracy: 0.7144 - val_loss: 0.8305\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 13ms/step - accuracy: 0.7678 - loss: 0.6751 - val_accuracy: 0.7214 - val_loss: 0.8103\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 12ms/step - accuracy: 0.7895 - loss: 0.6102 - val_accuracy: 0.7254 - val_loss: 0.8005\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.8131 - loss: 0.5548 - val_accuracy: 0.7305 - val_loss: 0.8017\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - accuracy: 0.8361 - loss: 0.4811 - val_accuracy: 0.7412 - val_loss: 0.7715\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7435 - loss: 0.7647\n",
            "테스트 정확도: 0.7411999702453613\n",
            "테스트 손실: 0.7714968323707581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vgg16_model(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # VGG16의 특징: Conv 블록과 MaxPooling\n",
        "    for i in range(3):  # VGG16의 기본 블록 3개\n",
        "        model.add(\n",
        "            layers.Conv2D(\n",
        "                filters=hp.Choice(f\"filters_block_{i}\", [64, 128, 256]),\n",
        "                kernel_size=3,\n",
        "                padding=\"same\",\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        model.add(\n",
        "            layers.Conv2D(\n",
        "                filters=hp.Choice(f\"filters_block_{i}\", [64, 128, 256]),\n",
        "                kernel_size=3,\n",
        "                padding=\"same\",\n",
        "                activation=\"relu\",\n",
        "            )\n",
        "        )\n",
        "        model.add(layers.MaxPooling2D(pool_size=2, strides=2))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully Connected 레이어\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            units=hp.Int(\"dense_units\", min_value=256, max_value=512, step=128),\n",
        "            activation=\"relu\",\n",
        "        )\n",
        "    )\n",
        "    model.add(layers.Dropout(rate=hp.Float(\"dropout_rate\", 0.2, 0.5, step=0.1)))\n",
        "\n",
        "    # 출력 레이어\n",
        "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "    # 컴파일 설정\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            learning_rate=hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n",
        "        ),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 튜너 설정 (랜덤 검색 또는 하이퍼밴드 중 선택)\n",
        "tuner = kt.Hyperband(\n",
        "    hypermodel=build_vgg16_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_epochs=10,  # 최대 에포크 수\n",
        "    factor=3,       # 자원 분배 비율\n",
        "    seed=42,\n",
        "    directory=\"vgg16_tuning\",\n",
        "    project_name=\"cifar10_vgg16\",\n",
        ")\n",
        "\n",
        "# 튜닝 시작\n",
        "tuner.search(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],\n",
        ")\n",
        "\n",
        "# 최적 하이퍼파라미터 출력\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"최적 하이퍼파라미터:\")\n",
        "for param, value in best_hps.values.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "\n",
        "# 최적 모델 재학습\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=20,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)],\n",
        ")\n",
        "\n",
        "# 테스트 데이터 평가\n",
        "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
        "print(f\"테스트 정확도: {test_acc}\")\n",
        "print(f\"테스트 손실: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BEefgOannVR",
        "outputId": "0a40cbe4-8760-405b-c550-dc7a6296aeba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 26 Complete [00h 04m 56s]\n",
            "val_accuracy: 0.751800000667572\n",
            "\n",
            "Best val_accuracy So Far: 0.7681000232696533\n",
            "Total elapsed time: 00h 36m 32s\n",
            "\n",
            "Search: Running Trial #27\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "256               |128               |filters_block_0\n",
            "64                |64                |filters_block_1\n",
            "256               |128               |filters_block_2\n",
            "512               |384               |dense_units\n",
            "0.3               |0.2               |dropout_rate\n",
            "0.01              |0.001             |learning_rate\n",
            "10                |10                |tuner/epochs\n",
            "0                 |4                 |tuner/initial_epoch\n",
            "0                 |2                 |tuner/bracket\n",
            "0                 |2                 |tuner/round\n",
            "\n",
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 25ms/step - accuracy: 0.1016 - loss: 14.8041 - val_accuracy: 0.1000 - val_loss: 2.3047\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 21ms/step - accuracy: 0.1011 - loss: 2.3042 - val_accuracy: 0.1000 - val_loss: 2.3041\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 22ms/step - accuracy: 0.0988 - loss: 2.3041 - val_accuracy: 0.1000 - val_loss: 2.3038\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 21ms/step - accuracy: 0.1029 - loss: 2.3035 - val_accuracy: 0.1000 - val_loss: 2.3038\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.0966 - loss: 2.3041 - val_accuracy: 0.1000 - val_loss: 2.3029\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.1010 - loss: 2.3037 - val_accuracy: 0.1000 - val_loss: 2.3047\n",
            "Epoch 7/10\n",
            "\u001b[1m 737/1563\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 20ms/step - accuracy: 0.1030 - loss: 2.3044"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eo5BFkv5qY5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "온10기/이국한/코어/서울\n",
        "이번에 퀘스트를 진행하게 되면서 하이퍼파라미터 자동화 즉 튜너 기능에 대해 다뤄 볼 수 있었습니다. 다만 a-z까지 직접 코드를 입력하기보다는 어느정도 초안을 받아 튜너 종류를 바꿔서 사용해봤고 사용함에 따라 어떤 부분들이 수정되어 바뀌어나가는지 또 이렇게 탐색해 나가는 과정에서 앞에서 다루어 봤던 모델도 한번 적용해 보면서 좀 더 편해진 부분도 있지만 실질적으로 제대로 알고 쓰지 않으면 단순히 시간만 길어지고 비용만  늘어나게되는 부분도 많이 있다는 것을 알게되었습니다.\n",
        "\n",
        "\n",
        "\n",
        "온10기/오병철/코어/경기\n",
        "회고: 딥러닝 모델을 학습하면서 하이퍼파라미터 튜닝의 중요성을 깨달았고, 특히 학습률과 배치 크기가 모델 성능에 큰 영향을 미친다는 점이 인상적이었습니다. ResNet 구조를 배우면서 Skip Connection이라는 혁신적인 아이디어로 깊은 신경망의 학습 문제를 해결할 수 있다는 점이 흥미로웠습니다. 실제 모델 구현과 튜닝 과정에서 71%의 정확도를 달성했는데, 이론적 지식을 실제 구현으로 옮기는 과정에서 많은 시행착오와 배움이 있었습니다."
      ],
      "metadata": {
        "id": "8FZ78izLxDeG"
      }
    }
  ]
}