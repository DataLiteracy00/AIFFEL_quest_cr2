{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBvAKjjsgv8D",
        "outputId": "0aad27b1-0480-41c8-bf86-56e4b67ecc0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 Complete [00h 52m 05s]\n",
            "val_accuracy: 0.6567999720573425\n",
            "\n",
            "Best val_accuracy So Far: 0.6567999720573425\n",
            "Total elapsed time: 00h 52m 05s\n",
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "48                |32                |initial_filters\n",
            "7                 |5                 |initial_kernel\n",
            "2                 |3                 |n_blocks\n",
            "0.4               |0.2               |dropout\n",
            "0.0020706         |0.00013571        |learning_rate\n",
            "\n",
            "Epoch 1/5\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 285ms/step - accuracy: 0.3898 - loss: 1.6750 - val_accuracy: 0.5520 - val_loss: 1.2629\n",
            "Epoch 2/5\n",
            "\u001b[1m356/704\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 268ms/step - accuracy: 0.5771 - loss: 1.1767"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras_tuner import RandomSearch\n",
        "import numpy as np\n",
        "\n",
        "# ResNet 블록 정의\n",
        "def resnet_block(x, filters, kernel_size=3, stride=1, conv_shortcut=True):\n",
        "    shortcut = x\n",
        "\n",
        "    if conv_shortcut:\n",
        "        shortcut = layers.Conv2D(filters, 1, strides=stride)(shortcut)\n",
        "        shortcut = layers.BatchNormalization()(shortcut)\n",
        "\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Add()([shortcut, x])\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# 하이퍼파라미터 튜닝을 위한 모델 빌더\n",
        "def build_model(hp):\n",
        "    inputs = keras.Input(shape=(32, 32, 3))\n",
        "\n",
        "    # 초기 컨볼루션 레이어\n",
        "    x = layers.Conv2D(\n",
        "        hp.Int('initial_filters', 32, 64, step=16),  # 1. Unit size 튜닝\n",
        "        kernel_size=hp.Choice('initial_kernel', [3, 5, 7]),  # 2. Kernel size 튜닝\n",
        "        strides=2,\n",
        "        padding='same'\n",
        "    )(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "    # ResNet 블록\n",
        "    for i in range(hp.Int('n_blocks', 2, 4)):  # 3. 레이어 수 튜닝\n",
        "        x = resnet_block(x, 64 * (2 ** i))\n",
        "\n",
        "    # 글로벌 풀링\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Dropout\n",
        "    dropout_rate = hp.Float('dropout', 0.2, 0.5, step=0.1)  # 4. Dropout rate 튜닝\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # 출력 레이어\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # 옵티마이저 및 학습률 튜닝\n",
        "    learning_rate = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')  # 5. Learning rate 튜닝\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# 데이터 로드 및 전처리\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# 검증 세트 분리\n",
        "val_size = 5000\n",
        "x_val = x_train[-val_size:]\n",
        "y_val = y_train[-val_size:]\n",
        "x_train = x_train[:-val_size]\n",
        "y_train = y_train[:-val_size]\n",
        "\n",
        "# GPU 확인\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# 하이퍼파라미터 튜닝 실행\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    directory='cifar10_resnet_tuning',\n",
        "    project_name='cifar10_resnet'\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=3)]\n",
        ")\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"\\n최적의 하이퍼파라미터:\")\n",
        "print(f\"Initial filters: {best_hps.get('initial_filters')}\")\n",
        "print(f\"Initial kernel size: {best_hps.get('initial_kernel')}\")\n",
        "print(f\"Number of ResNet blocks: {best_hps.get('n_blocks')}\")\n",
        "print(f\"Dropout rate: {best_hps.get('dropout')}\")\n",
        "print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n",
        "\n",
        "# 최적의 모델로 최종 평가\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "evaluation = best_model.evaluate(x_test, y_test)\n",
        "print(f\"\\n테스트 세트 성능:\")\n",
        "print(f\"Loss: {evaluation[0]:.4f}\")\n",
        "print(f\"Accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "# 학습 곡선 시각화\n",
        "best_model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[keras.callbacks.EarlyStopping(patience=5)],\n",
        ")\n"
      ]
    }
  ]
}